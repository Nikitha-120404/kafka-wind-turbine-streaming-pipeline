# -------------------------------------------------------------
# Kafka Producer - Wind Turbine Data Ingestion
# -------------------------------------------------------------
# This script reads simulated wind turbine data from a log file
# and publishes each JSON record to an Apache Kafka topic.
#
# Flow:
# wind_turbine.log  -->  Kafka Producer  -->  Kafka Broker
# -------------------------------------------------------------

# Import Kafka Producer from Confluent Kafka library
# Install using: pip install confluent_kafka
from confluent_kafka import Producer

# Import standard Python libraries
import json      # For reading and writing JSON data
import time      # (Optional) for delays if needed

# -------------------------------------------------------------
# Kafka Configuration
# -------------------------------------------------------------

# Kafka broker configuration
conf = {
    'bootstrap.servers': '192.168.59.128:9092',   # Kafka broker IP + port
    'client.id': 'wind_turbine_fullfile_producer' # Unique producer ID
}

# Create Kafka producer instance
producer = Producer(conf)

# Kafka topic name
TOPIC_NAME = "windturbine-data"

# Example topic creation command (run on Kafka server):
# bin/kafka-topics.sh --create \
# --topic <topic_name> \
# --bootstrap-server <broker_host>:9092 \
# --partitions <n> \
# --replication-factor <n>

# -------------------------------------------------------------
# Log File Configuration
# -------------------------------------------------------------

# Input log file generated by sensor simulation script
log_file_path = "wind_turbine.log"

# -------------------------------------------------------------
# Kafka Delivery Callback
# -------------------------------------------------------------

# Callback function to confirm message delivery
def delivery_report(err, msg):
    if err:
        print(f"Kafka delivery failed: {err}")
    else:
        print(f"Kafka message delivered to {msg.topic()} [{msg.partition()}]")

# -------------------------------------------------------------
# Kafka Message Production
# -------------------------------------------------------------

try:
    # Open log file and read line-by-line
    with open(log_file_path, "r") as logfile:

        for line in logfile:
            line = line.strip()

            # Skip empty lines
            if not line:
                continue

            try:
                # Convert JSON string into Python dictionary
                json_data = json.loads(line)

                # Send message to Kafka topic
                producer.produce(
                    TOPIC_NAME,
                    value=json.dumps(json_data),
                    callback=delivery_report
                )

                # Trigger delivery callback (non-blocking)
                producer.poll(0)

            except json.JSONDecodeError:
                # Skip invalid JSON lines
                print(f"Invalid JSON line skipped: {line}")

    # Ensure all messages are delivered
    producer.flush()
    print("All messages sent!")

# -------------------------------------------------------------
# Graceful Exit (Ctrl + C)
# -------------------------------------------------------------

except KeyboardInterrupt:
    print("\nStopping ingestion...")
    producer.flush()

# END OF CODE
